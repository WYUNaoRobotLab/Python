{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\for_anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import  ImageDraw, Image\n",
    "from tensorflow.python import pywrap_tensorflow\n",
    "from tensorflow.python import debug as tfdbg\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import math\n",
    "\n",
    "def random_mini_batches(image, anchor_label,true_box_label, prior_boxes,mini_batch_size=64, seed=0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (image, anchor_label,true_box_label)\n",
    "\n",
    "    Arguments:\n",
    "    image -- matrix of shape (None ,256 ,256,3)\n",
    "    anchor_label --  of shape (None, grid,gird,num_anchor_per_cell,5+num_classes)\n",
    "    true_box_label -- of shape ( None, num_boxes ,5)\n",
    "    prior_boxes -- of shape(None,grid,grid,num_anchor_per_cell,4)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
    "\n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "\n",
    "    m = image.shape[0]  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Step 1: Shuffle (image,anchor_label, true_box_label)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_image = image[permutation]\n",
    "    shuffled_anchor = anchor_label[permutation]\n",
    "    shuffled_true_box = true_box_label[permutation]\n",
    "    shuffled_prior_box = prior_boxes[permutation]\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(\n",
    "        m / mini_batch_size)  # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_image = shuffled_image[k * mini_batch_size: k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch_anchor = shuffled_anchor[ k * mini_batch_size: k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch_true_box = shuffled_true_box[k*mini_batch_size :k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch_prior_box = shuffled_prior_box[ k * mini_batch_size: k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_image, mini_batch_anchor, mini_batch_true_box, mini_batch_prior_box)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_image = shuffled_image[num_complete_minibatches * mini_batch_size: m]\n",
    "        mini_batch_anchor = shuffled_anchor[ num_complete_minibatches * mini_batch_size: m]\n",
    "        mini_batch_true_box = shuffled_true_box[num_complete_minibatches * mini_batch_size: m]\n",
    "        mini_batch_prior_box = shuffled_prior_box[ k * mini_batch_size: k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_image, mini_batch_anchor, mini_batch_true_box, mini_batch_prior_box)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    return mini_batches\n",
    "\n",
    "\n",
    "def readh5(h5_path ,amount):\n",
    "    f = h5py.File(h5_path, 'r')\n",
    "    train_images = np.array(f['images'][amount[0]:amount[1]])\n",
    "    anchor_labels = np.array(f['anchor_labels'][amount[0]:amount[1]])\n",
    "    true_box_labels = np.array(f[\"true_box_labels\"][amount[0]:amount[1]])\n",
    "    prior_boxes = np.array(f[\"prior_boxes\"][amount[0]:amount[1]])\n",
    "    f.close()\n",
    "    return train_images, anchor_labels, true_box_labels ,prior_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Yolo():\n",
    "    def __init__(self,num_true_boxes,num_anchor_per_box,img_width,img_height,training_able):\n",
    "        self.save_list = []\n",
    "        self.obj_cost_scale = 6 \n",
    "        self.obj_location_scale = 5\n",
    "        self.num_true_boxes = num_true_boxes # 每张图片的true_boxes 的数量的最大值\n",
    "        self.num_anchor_per_box = num_anchor_per_box\n",
    "        self.img_width = img_width\n",
    "        self.img_height = img_height\n",
    "        self.training_able = training_able\n",
    "    def my_conv(self, name, shape, input_data, strides, padding, training_able=True, init_filter=None, init_gama = None,init_beta = None,skip_item =None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param name: name of filter\n",
    "        :param shape: shape of filter\n",
    "        :param input_data: data will be convlutioned\n",
    "        :param strides: strides of convlution\n",
    "        :param padding: \"valid\" or \"SAME\"\n",
    "        :return leaky_conv:      result of convlutional layer\n",
    "        \"\"\"\n",
    "        if init_filter is None:\n",
    "            filter_ = tf.get_variable(name=name, shape=shape, dtype=tf.float32,\n",
    "                                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                      trainable=training_able)\n",
    "        else:\n",
    "            filter_ = tf.get_variable(name=name, dtype=tf.float32, initializer=init_filter, trainable=training_able)\n",
    "        conv_ = tf.nn.conv2d(input_data, filter_, strides=strides, padding=padding)\n",
    "        mean, variance =tf.nn.moments(conv_,axes=[0,1,2],shift=None,name=None,keep_dims=False)\n",
    "        if init_gama is None:\n",
    "            gama = tf.get_variable(name = \"gama\" + name.split(\"_\")[-1],initializer=tf.ones(shape = variance.shape))\n",
    "        else:\n",
    "            gama = tf.get_variable(name = \"gama\" + name.split(\"_\")[-1],initializer=init_gama,trainable=training_able)\n",
    "        if init_beta is None:\n",
    "            beta = tf.get_variable(name = \"beta\" + name.split(\"_\")[-1],initializer=tf.zeros(shape=mean.shape))\n",
    "        else:\n",
    "            beta = tf.get_variable(name = \"beta\" + name.split(\"_\")[-1],initializer=init_beta,trainable=training_able)\n",
    "        if skip_item is not None:\n",
    "            z_ =tf.nn.batch_normalization(variance_epsilon=1e-6,x =conv_+skip_item,mean=mean,variance=variance,offset=beta,scale=gama)\n",
    "        else:\n",
    "            z_ =tf.nn.batch_normalization(variance_epsilon=1e-6,x =conv_,mean=mean,variance=variance,offset=beta,scale=gama)\n",
    "        leaky_conv = tf.nn.leaky_relu(z_, 0.3)\n",
    "        self.save_list.append(filter_)\n",
    "        self.save_list.append(beta)\n",
    "        self.save_list.append(gama)\n",
    "        return leaky_conv\n",
    "    \n",
    "    def my_conv_no_bn(self, name, shape, input_data, strides, padding, training_able=True, init_filter=None,skip_item =None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param name: name of filter\n",
    "        :param shape: shape of filter\n",
    "        :param input_data: data will be convlutioned\n",
    "        :param strides: strides of convlution\n",
    "        :param padding: \"valid\" or \"SAME\"\n",
    "        :return leaky_conv:      result of convlutional layer\n",
    "        \"\"\"\n",
    "        if init_filter is None:\n",
    "            filter_ = tf.get_variable(name=name, shape=shape, dtype=tf.float32,\n",
    "                                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                      trainable=training_able)\n",
    "        else:\n",
    "            filter_ = tf.get_variable(name=name, dtype=tf.float32, initializer=init_filter, trainable=training_able)\n",
    "        conv_ = tf.nn.conv2d(input_data, filter_, strides=strides, padding=padding)\n",
    "       \n",
    "        \n",
    "        if skip_item is not None:\n",
    "            z_ = skip_item + conv_\n",
    "        else:\n",
    "            z_ = conv_\n",
    "        leaky_conv = tf.nn.leaky_relu(z_, 0.1)\n",
    "        self.save_list.append(filter_)\n",
    "        \n",
    "        return leaky_conv\n",
    "\n",
    "    def train(self, training_data, training_label,training_treu_boxes, training_prior_boxes,classes, learning_rate, minibatch_size, num_epochs,anchor_box,reader):\n",
    "        \"\"\"\n",
    "\n",
    "        :param training_data: shape(None,416,416,3)\n",
    "        :param training_label: shape(None,13,13,6,8)\n",
    "        :training_true_boxes:  shape(None,num_true_box,5) 5:true_x,true_y,true_w, true_h, which class\n",
    "        :prior_boxes:          shape(None,13,13,6,4)\n",
    "        :param classes:        scale ,3\n",
    "        :param learning_rate:   \n",
    "        :param minibatch_size:\n",
    "        :param num_epochs:\n",
    "        :param anchor_box_tensor:   (num_anchor_boxes_per_cell,2), 以32像素为单位长度\n",
    "                                     \n",
    "        :return:\n",
    "        \"\"\"\n",
    "        input_img = tf.placeholder(dtype=tf.float32, shape=[None, self.img_height, self.img_width, 3], name=\"input_img\")\n",
    "        input_label = tf.placeholder(dtype=tf.float32, shape=[None, self.img_height/32, self.img_width/32, self.num_anchor_per_box, 5 + classes], name=\"input_label\")\n",
    "        true_boxes = tf.placeholder(dtype=tf.float32, shape=[None, self.num_true_boxes, 5], name=\"true_boxes\")\n",
    "        prior_boxes = tf.placeholder(dtype = tf.float32,shape =[None,self.img_height/32, self.img_width/32,self.num_anchor_per_box,4],name=\"prior_boxes\")\n",
    "        anchor_box_tensor =tf.constant(value = anchor_box)\n",
    "        conv_9,conv_8 = self.backbone(input_img,reader,self.training_able)\n",
    "        conv_15 = self.yolo_head(conv_9,conv_8,classes)\n",
    "        box_xy, box_wh, box_confidence, class_probability = self.detection(conv_15,num_calsses = classes,anchor_box_tensor = anchor_box_tensor)\n",
    "        iou_mask, detection_mask =self.preprocess_true_box(true_boxes,box_wh,box_xy,input_label,threshold_iou = 0.5)\n",
    "        total_cost = self.calculate_cost(iou_mask,detection_mask,box_confidence,box_xy,box_wh,input_label,class_probability,prior_boxes)\n",
    "        global_step = tf.Variable(0,trainable = False)\n",
    "        decay_learning_rate = tf.train.exponential_decay(learning_rate, global_step, 40, 0.90, staircase=True)\n",
    "        optimizer = tf.train.AdamOptimizer(decay_learning_rate).minimize(total_cost,global_step)\n",
    "        saver = tf.train.Saver(\n",
    "            self.save_list, max_to_keep=51)\n",
    "        config = tf.ConfigProto(log_device_placement=True, allow_soft_placement=True)\n",
    "        with tf.Session(config=config) as sess:\n",
    "\n",
    "            costs = []\n",
    "            initzer = tf.global_variables_initializer()\n",
    "            sess.run(initzer)\n",
    "            num_minibatches = int(\n",
    "                training_data.shape[\n",
    "                    0] / minibatch_size)  # number of minibatches of size minibatch_size in the train set\n",
    "            seed = 0\n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "                epoch_cost = 0\n",
    "                seed += 1\n",
    "                mini_batches = random_mini_batches(training_data, training_label, training_treu_boxes,training_prior_boxes, minibatch_size,\n",
    "                                                   seed=seed)\n",
    "                for batch in mini_batches:\n",
    "                    _, minibatch_cost = sess.run([optimizer, total_cost],\n",
    "                                                 feed_dict={input_img: batch[0] / 255, input_label: batch[1],\n",
    "                                                            true_boxes: batch[2],prior_boxes:batch[3]})\n",
    "                    epoch_cost += minibatch_cost\n",
    "                epoch_cost /= num_minibatches\n",
    "                if epoch % 20 == 0:\n",
    "                    print(epoch_cost)\n",
    "                    costs.append(epoch_cost)\n",
    "                    save_path = saver.save(sess, r\"D:\\YOLOv3\\save_dir9\\model.ckpt\",\n",
    "                                           global_step=epoch)\n",
    "\n",
    "            plt.plot(np.squeeze(costs))\n",
    "            plt.ylabel('cost')\n",
    "            plt.xlabel('iterations (per tens)')\n",
    "            plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    def backbone(self,input_img,reader,training_able):\n",
    "\n",
    "        conv_1 = self.my_conv(name = \"filter_1\",shape = [3,3,3,32],input_data = input_img, strides = [1,1,1,1], \n",
    "                              padding =\"SAME\",training_able=self.training_able[0] ,init_filter = reader.get_tensor(\"filter_1\"),\n",
    "                             init_beta = reader.get_tensor(\"beta1\"),init_gama = reader.get_tensor(\"gama1\"))\n",
    "        print(\"conv_1 shpae is :\",conv_1.shape) # (None,416,416,32)\n",
    "        conv_2 = self.my_conv(name = \"filter_2\",shape = [3,3,32,64],input_data = conv_1, strides = [1,2,2,1],\n",
    "                              padding =\"SAME\",training_able=self.training_able[1],init_filter = reader.get_tensor(\"filter_2\"),\n",
    "                             init_beta = reader.get_tensor(\"beta2\"),init_gama = reader.get_tensor(\"gama2\"))\n",
    "        print(\"conv_2 shape is:\",conv_2.shape) # (None,208,208,64)\n",
    "        conv_3 = self.my_conv(name=\"filter_3\", shape=[1, 1, 64, 32], input_data=conv_2, strides=[1, 1, 1, 1],\n",
    "                              padding=\"VALID\",training_able=self.training_able[2],init_filter = reader.get_tensor(\"filter_3\"),\n",
    "                             init_beta = reader.get_tensor(\"beta3\"),init_gama = reader.get_tensor(\"gama3\"))\n",
    "        print(\"conv_3 shape is:\", conv_3.shape) # (None,208,208,32)\n",
    "        conv_4 = self.my_conv(name=\"filter_4\", shape=[3, 3, 32, 64], input_data=conv_3, strides=[1, 1, 1, 1],\n",
    "                              padding=\"SAME\",training_able=self.training_able[3],init_filter = reader.get_tensor(\"filter_4\"),\n",
    "                             init_beta = reader.get_tensor(\"beta4\"),init_gama = reader.get_tensor(\"gama4\"))\n",
    "        print(\"conv_4 shape is:\", conv_4.shape) # (None,208,208,64)\n",
    "        resi_1 = conv_2 + conv_4 # 残差项\n",
    "        conv_5 = self.my_conv(name=\"filter_5\", shape=[3, 3, 64, 128], input_data=resi_1, strides=[1, 2, 2, 1],\n",
    "                              padding=\"SAME\",training_able=self.training_able[4],init_filter = reader.get_tensor(\"filter_5\"),\n",
    "                             init_beta = reader.get_tensor(\"beta5\"),init_gama = reader.get_tensor(\"gama5\"))\n",
    "        print(\"conv_5 shape is:\", conv_5.shape) # (None,104,104,128)\n",
    "        conv_6 = self.my_conv(name = \"filter_6\", shape =[1,1,128,64], input_data=conv_5, strides=[1,1,1,1],\n",
    "                              padding=\"SAME\",training_able=self.training_able[5],init_filter = reader.get_tensor(\"filter_6\"),\n",
    "                             init_beta = reader.get_tensor(\"beta6\"),init_gama = reader.get_tensor(\"gama6\"))\n",
    "        print(\"conv_6 shape is:\",conv_6.shape) # (None,104,104,64)\n",
    "        conv_7 = self.my_conv(name=\"filter_7\", shape=[3, 3, 64, 128], input_data=conv_6, strides=[1, 1, 1, 1],\n",
    "                              padding=\"SAME\",training_able=self.training_able[6],init_filter = reader.get_tensor(\"filter_7\"),\n",
    "                             init_beta = reader.get_tensor(\"beta7\"),init_gama = reader.get_tensor(\"gama7\")) # (None,64,64,128)\n",
    "        print(\"conv_7 shape is:\", conv_7.shape) # (None,104,104,128)\n",
    "        resi_2 = conv_7 +conv_5\n",
    "\n",
    "        conv_8 =self.my_conv(name =\"filter_8\",shape=[3,3,128,64],input_data=resi_2,strides=[1,2,2,1],\n",
    "                             padding=\"SAME\",training_able=self.training_able[7],init_filter = reader.get_tensor(\"filter_8\"),\n",
    "                            init_beta = reader.get_tensor(\"beta8\"),init_gama = reader.get_tensor(\"gama8\"))\n",
    "        print(\"conv_8 shape is:\",conv_8.shape) # (None,52,52,64)\n",
    "        conv_9 = self.my_conv(name=\"filter_9\", shape=[3, 3, 64, 64], input_data=conv_8, strides=[1, 2, 2, 1],\n",
    "                              padding=\"SAME\",training_able=self.training_able[8],init_filter = reader.get_tensor(\"filter_9\"),\n",
    "                             init_beta = reader.get_tensor(\"beta9\"),init_gama = reader.get_tensor(\"gama9\"))\n",
    "        print(\"conv_9 shape is:\", conv_9.shape)  # (None,26,26,64)\n",
    "        \"\"\"\n",
    "        conv_10 = self.my_conv(name=\"filter_10\", shape=[3, 3, 16, 14], input_data=resi_2, strides=[1, 2, 2, 1],\n",
    "                              padding=\"SAME\")\n",
    "        print(\"conv_10 shape is:\", conv_10.shape)  # (None,8,8,14)\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        return conv_9 ,conv_8\n",
    "    # darknet 完成，下面开始写 yolo_head\n",
    "    def yolo_head(self,extract_result,skip_connection,classes):\n",
    "        conv_10 = self.my_conv_no_bn(name=\"filter_10\", shape=[1, 1, 64, 64], input_data=extract_result, strides=[1, 1, 1, 1],\n",
    "                              padding=\"SAME\")    # (None,26,26,64)\n",
    "        print(\"conv_10 shape is:\", conv_10.shape)\n",
    "        conv_11 = self.my_conv_no_bn(name=\"filter_11\",shape=[3,3,64,64],input_data=conv_10,strides=[1,1,1,1],\n",
    "                               padding=\"SAME\")   # (None,26,26,64)\n",
    "        print(\"conv_11 shape is:\", conv_11.shape)\n",
    "        #conv_skip_con = self.my_conv(name = \"skipfilter_01\",shape = [1,1,64,64],input_data = skip_connection,strides=[1,1,1,1],\n",
    "                                     #padding=\"SAME\")\n",
    "        #to_depth_skip_con = tf.space_to_depth(conv_skip_con,2) # (None,26,26,256)\n",
    "        #reorg = tf.concat(axis = -1,values = [conv_11,to_depth_skip_con])    # (None,26,26,320)\n",
    "        #print(\"reorg.shape is:\",reorg.shape)\n",
    "        conv_12 = self.my_conv_no_bn(name=\"filter_12\",shape=[1,1,64,128],input_data=conv_11,strides=[1,1,1,1],\n",
    "                               padding=\"SAME\")  # (None,26,26,128)\n",
    "        print(\"conv_12 shape is:\", conv_12.shape)\n",
    "        conv_13 = self.my_conv_no_bn(name=\"filter_13\", shape=[3, 3, 128, 128], input_data=conv_12, strides=[1, 2, 2, 1],\n",
    "                               padding=\"SAME\")  # (None,13,13,128)\n",
    "        print(\"conv_13 shape is:\", conv_13.shape)\n",
    "        conv_skip_con = self.my_conv_no_bn(name = \"skipfilter_01\",shape = [1,1,64,64],input_data = skip_connection,strides=[1,1,1,1],\n",
    "                                     padding=\"SAME\")\n",
    "        to_depth_skip_con = tf.space_to_depth(conv_skip_con,4) # (None,13,13,1024)\n",
    "        reorg = tf.concat(axis = -1,values = [conv_13,to_depth_skip_con])    # (None,13,13,1024+128)\n",
    "        print(\"reorg.shape is:\",reorg.shape)\n",
    "        #conv_14 = self.my_conv(name = \"filter_14\",shape = [1,1,64,64],input_data = skip_connection,strides = [1,1,1,1],padding = \"SAME\")\n",
    "        \n",
    "        filter_15 = tf.get_variable(name=\"filter_15\",shape = [1,1,1152,self.num_anchor_per_box*(classes + 5)],dtype = tf.float32,initializer=tf.contrib.layers.variance_scaling_initializer(mode='FAN_AVG',\n",
    "                                                                                                 uniform=True,\n",
    "                                                                                                 seed=None))\n",
    "        self.save_list.append(filter_15)\n",
    "        conv_15 = tf.nn.conv2d(reorg, filter_15, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        # (None,13,13,?)\n",
    "        print(\"conv_15 shape is:\", conv_15.shape)\n",
    "        return conv_15\n",
    "\n",
    "        # yolo_head 完成,开始detection ,得到输出\n",
    "    def detection(self,feat,num_calsses,anchor_box_tensor):\n",
    "        grid_dims = tf.stack([feat.shape[2],feat.shape[1]]) # width, height\n",
    "        grid_width_index = tf.cast(tf.range(grid_dims[0]),tf.float32)\n",
    "        grid_height_index = tf.cast(tf.range(grid_dims[1]),tf.float32)\n",
    "        grid_width_index ,grid_height_index = tf.meshgrid(grid_width_index,grid_height_index)\n",
    "        grid_width_index = tf.reshape(grid_width_index,[-1,1])\n",
    "        grid_height_index = tf.reshape(grid_height_index,[-1,1])\n",
    "        grid_wh_index = tf.concat(values =[grid_width_index,grid_height_index],axis=1) # shape (8*8,2)\n",
    "        # 0,0\n",
    "        # 1,0\n",
    "        # ...\n",
    "        # 7,0\n",
    "        # 0,1\n",
    "        # 1,1\n",
    "        # ...\n",
    "        # 7,7\n",
    "        print(grid_wh_index.shape)\n",
    "        grid_wh_index = tf.reshape(grid_wh_index,[1,grid_dims[1],grid_dims[0],1,2])\n",
    "        \n",
    "        \n",
    "        feat = tf.reshape(feat,[-1,feat.shape[1],feat.shape[2],self.num_anchor_per_box,5+num_calsses]) # 8 是 anchor boxes 的数量\n",
    "        box_xy = tf.sigmoid(feat[...,1:3])\n",
    "        box_xy = (box_xy + grid_wh_index) /tf.cast(grid_dims,tf.float32)                              # bx = (sigmoid(tx) + cx)/grid_width\n",
    "                                                                                         # by 同理\n",
    "        anchor_box_tensor = tf.reshape(anchor_box_tensor,shape=[1,1,1,anchor_box_tensor.shape[0],2]) # anchor_box_tensor shape (1,1,1,num_anchor_per_grid,2)\n",
    "\n",
    "        box_wh = tf.exp(feat[...,3:5])* anchor_box_tensor                                         # box_width = exp(tw)*anchor_width 这里之所以不除以grid_width，\n",
    "                                                                                                     # 是因为anchor_box_tensor已经归一化到[0,1]区间\n",
    "        box_confidence = tf.sigmoid(feat[...,0:1]) # 得到 confidence\n",
    "        #box_confidence = tf.reshape(box_confidence,shape=[box_confidence.shape+(1,))  # shape(batch, grid, grid, num_anchor_per_box,1)\n",
    "        print(\"box_confidence shape is\",box_confidence.shape)\n",
    "        class_probability = tf.sigmoid(feat[...,5:])\n",
    "        return box_xy, box_wh, box_confidence, class_probability\n",
    "        # detection 完成，得到 xy,wh,confidence, probability\n",
    "    def preprocess_true_box(self,true_boxes, box_wh, box_xy, input_label, threshold_iou =0.5):\n",
    "        # 参考过GitHub上的开源代码和吴恩达课程作业的代码，会发现有matching_true_box，matching_box, matching_class\n",
    "        # 这些变量，这是为了计算cost做准备 (class cost 和 xy 坐标, wh大小的cost),但我并没有计算上述的这三个变量，\n",
    "        # 因为这些变量要表达的信息是图像里的true_box 落在哪一个grid_cell，x,y,w,h 和类别，而这些信息\n",
    "        # 在将anchor_label写入h5前，就包含在anchor_label里 了\n",
    "        # preprocess true box 开始，主要是为得到掩码用于区分有object和没有object(noobject)\n",
    "\n",
    "            # 计算 预测得到的box与 true_box(ground truth box)的iou\n",
    "                # true_boxes shape ( batch, num_true_box,5);5 意味着(x,y,w,h,class) num_true_box的数值为各图片中true_boxes的数量的最大值\n",
    "        true_boxes_shape = tf.shape(true_boxes)\n",
    "        true_boxes = tf.reshape(true_boxes,[true_boxes_shape[0],1,1,1,true_boxes_shape[1],true_boxes_shape[2]])\n",
    "        # shape is (batch,1,1,1,num_true_box,5)\n",
    "        true_box_wh_half = true_boxes[...,2:4]/2\n",
    "        true_left_upper = true_boxes[...,0:2] - true_box_wh_half\n",
    "\n",
    "        true_right_bottom = true_boxes[..., 0:2] + true_box_wh_half\n",
    "\n",
    "        box_wh = tf.expand_dims(box_wh,4) # shpae (batch, grid,grid,num_anchor_per_cell,1,2)\n",
    "        box_xy = tf.expand_dims(box_xy, 4)\n",
    "        pred_box_wh_half = box_wh/2\n",
    "        pred_left_upper = box_xy - pred_box_wh_half\n",
    "\n",
    "        pred_right_bottom = box_xy + pred_box_wh_half\n",
    "\n",
    "\n",
    "        # 下面的这个比较，应该是先进行broadcasting,各自被广播成(batch,grid,grid,num_anchor_per_cell,num_true_box,2)\n",
    "        # 广播的概念与numpy的相同\n",
    "        inter_min = tf.maximum(true_left_upper,pred_left_upper)\n",
    "        inter_max = tf.minimum(true_right_bottom,pred_right_bottom)\n",
    "        inter_wh = tf.maximum(inter_max - inter_min,0)\n",
    "        intersection_area = inter_wh[...,0] * inter_wh[...,1]\n",
    "        # 求交集的代码也可写成下面那样\n",
    "        \"\"\"\n",
    "        xmin = tf.maximum(true_left_upper[...,0],pred_left_upper[...,0])\n",
    "        ymin = tf.maximum(true_left_upper[...,1],pred_left_upper[...,1])\n",
    "        xmax = tf.minimum(true_right_bottom[...,0],pred_right_bottom[...,0])\n",
    "        ymax = tf.minimum(true_right_bottom[...,1],pred_right_bottom[...,1])\n",
    "        width_difference = tf.maximum(xmax - xmin,0)\n",
    "        height_difference = tf.maximum(ymax - ymin,0)\n",
    "        intersection_area = width_difference * height_difference\n",
    "        \"\"\"\n",
    "        true_area = true_boxes[...,2] * true_boxes[...,3]\n",
    "        pred_area = box_wh[...,0] * box_wh[...,1]\n",
    "        union_area = true_area + pred_area - intersection_area\n",
    "        iou = intersection_area / union_area        # shape (batch,grid,grid,num_anchor_per_cell,num_true_box)\n",
    "        best_iou = tf.reduce_max(iou,axis = -1,keepdims = True) # reduce 意味着沿着axis轴减少一个维度,即减少axis轴那个维度（除非 keepdims is True）\n",
    "                                                                # axis选为最后一个维度,意味着对每一个pred_box而言，从num_true_box中，选出一个与pred_box最接近的）\n",
    "        iou_mask = tf.cast(best_iou>threshold_iou,dtype = tf.float32) # 用以判断anchor box 是否检测到object\n",
    "                                                                      # shape is (batch,grid,grid,num_anchor_per_cell,1)\n",
    "\n",
    "        # 下面要得到一个数据:detection_mask, 通过它可知每一个gridcell,对应哪一个anchor\n",
    "        detection_mask = input_label[...,0:1] # 通过 confidence 获知，哪个anchor负责该gridcell，但这里并没有保证每一个gridcell最多只有一个anchor为1，\n",
    "                                            # 要保证也可以，但还是应该通过获取图片，标注label时,检查每个gridcell的中心数是否大于1\n",
    "                                            # shape is (batch,grid,grid,num_anchor_per_cell)\n",
    "        #detection_mask = tf.reshape(detection_mask,shape= detection_mask.shape+(1,))  # shape is (batch,grid,grid,num_anchor_per_cell,1)\n",
    "        print(\"detection_mask is:\",detection_mask.shape)\n",
    "        return iou_mask, detection_mask\n",
    "\n",
    "\n",
    "    def calculate_cost(self, iou_mask, detection_mask, box_confidence,box_xy,box_wh,anchor_label, class_probability,prior_boxes,lambd = 0.1):\n",
    "        no_obj_conf_cost = (1-iou_mask)*(1-detection_mask)*tf.square(box_confidence)\n",
    "        obj_conf_cost = detection_mask*tf.square(1-box_confidence)*self.obj_cost_scale\n",
    "        conf_cost = tf.reduce_sum(no_obj_conf_cost + obj_conf_cost) # confidence cost\n",
    "\n",
    "        # probability cost\n",
    "        prob_cost = tf.reduce_sum(detection_mask * tf.square(anchor_label[...,5:] - class_probability))\n",
    "\n",
    "        # center cost\n",
    "        \n",
    "        xy_cost = self.obj_location_scale*tf.reduce_sum((4-1.5*anchor_label[...,3:4]*anchor_label[...,4:5])*detection_mask*tf.square(anchor_label[...,1:3] - box_xy))\n",
    "        \n",
    "        # height width cost \n",
    "        wh_cost = self.obj_location_scale*tf.reduce_sum((4-1.5*anchor_label[...,3:4]*anchor_label[...,4:5])*detection_mask*tf.square(anchor_label[...,3:5]-box_wh))\n",
    "        \n",
    "        # height width cost between prior and pred box\n",
    "        #prior_pred_xy_cost = tf.reduce_sum(detection_mask*tf.square(prior_boxes[...,0:2] - box_xy))\n",
    "        \n",
    "        # center cost between prior and pred box\n",
    "        #prior_pred_wh_cost = tf.reduce_sum(detection_mask*tf.square(prior_boxes[...,2:4] - box_wh))\n",
    "        # total cost\n",
    "        total_cost = conf_cost + prob_cost + xy_cost + wh_cost \n",
    "        return total_cost\n",
    "\n",
    "\n",
    "\n",
    "    def retrain(self, training_data, training_anchor_label, training_treu_boxes,training_prior_boxes,classes, learning_rate, minibatch_size, num_epochs,anchor_box,reader,lambd=0.1):\n",
    "        input_img = tf.placeholder(dtype=tf.float32, shape=[None, self.img_height, self.img_width, 3], name=\"input_img\")\n",
    "        input_label = tf.placeholder(dtype=tf.float32, shape=[None, self.img_height/32, self.img_width/32,self.num_anchor_per_box , 5 + classes], name=\"input_label\")\n",
    "        true_boxes = tf.placeholder(dtype=tf.float32, shape=[None, self.num_true_boxes, 5], name=\"true_boxes\")\n",
    "        prior_boxes = tf.placeholder(dtype = tf.float32,shape =[None,self.img_height/32, self.img_width/32,self.num_anchor_per_box,4],name=\"prior_boxes\")\n",
    "        anchor_box_tensor =tf.constant(value = anchor_box)\n",
    "        conv_9,conv_8 = self.backbone_for_retrain(input_img,reader,self.training_able)\n",
    "        conv_15 = self.yolo_head_for_retrain(conv_9,conv_8,reader)\n",
    "        box_xy, box_wh, box_confidence, class_probability = self.detection(conv_15, num_calsses=classes,\n",
    "                                                                           anchor_box_tensor=anchor_box_tensor)\n",
    "        iou_mask, detection_mask = self.preprocess_true_box(true_boxes, box_wh, box_xy, input_label, threshold_iou=0.5)\n",
    "        total_cost = self.calculate_cost(iou_mask, detection_mask, box_confidence, box_xy, box_wh, input_label,\n",
    "                                         class_probability,prior_boxes,lambd)\n",
    "        \n",
    "        global_step = tf.Variable(0,trainable = False)\n",
    "        decay_learning_rate = tf.train.exponential_decay(learning_rate, global_step, 40, 0.9, staircase=False)\n",
    "        optimizer = tf.train.AdamOptimizer(decay_learning_rate).minimize(total_cost,global_step)\n",
    "        saver = tf.train.Saver(\n",
    "            self.save_list, max_to_keep=30)\n",
    "        config = tf.ConfigProto(log_device_placement=True, allow_soft_placement=True)\n",
    "        \n",
    "        \n",
    "        with tf.Session(config=config) as sess:\n",
    "           \n",
    "            initzer = tf.global_variables_initializer()\n",
    "            sess.run(initzer)\n",
    "            costs = []\n",
    "            \n",
    "            num_minibatches = int(\n",
    "                training_data.shape[\n",
    "                    0] / minibatch_size)  # number of minibatches of size minibatch_size in the train set\n",
    "            seed = 0\n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "                epoch_cost = 0\n",
    "                seed += 1\n",
    "                mini_batches = random_mini_batches(training_data, training_anchor_label, training_treu_boxes,training_prior_boxes, minibatch_size,\n",
    "                                                   seed=seed)\n",
    "                for batch in mini_batches:\n",
    "                    _, minibatch_cost = sess.run([optimizer, total_cost],\n",
    "                                                 feed_dict={input_img: batch[0] / 255, input_label: batch[1],\n",
    "                                                            true_boxes: batch[2],prior_boxes:batch[3]})\n",
    "                    epoch_cost += minibatch_cost\n",
    "                epoch_cost /= num_minibatches\n",
    "                if epoch % 10 == 0:\n",
    "                    print(epoch_cost)\n",
    "                    costs.append(epoch_cost)\n",
    "                    save_path = saver.save(sess, r\"D:\\YOLOv3_2\\test_samples_copy2\\model.ckpt\",\n",
    "                                           global_step=epoch)\n",
    "            plt.plot(np.squeeze(costs))\n",
    "            plt.ylabel('cost')\n",
    "            plt.xlabel('iterations (per tens)')\n",
    "            plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "            plt.show()\n",
    "    def backbone_for_retrain(self,input_img,reader,training_able):\n",
    "        \n",
    "        conv_1 = self.my_conv(name = \"filter_1\",shape = [3,3,3,32],input_data = input_img, strides = [1,1,1,1], \n",
    "                              padding =\"SAME\",training_able=self.training_able[0] ,init_filter = reader.get_tensor(\"filter_1\"),\n",
    "                             init_beta = reader.get_tensor(\"beta1\"),init_gama = reader.get_tensor(\"gama1\"))\n",
    "        print(\"conv_1 shpae is :\",conv_1.shape) # (None,416,416,32)\n",
    "        conv_2 = self.my_conv(name = \"filter_2\",shape = [3,3,32,64],input_data = conv_1, strides = [1,2,2,1],\n",
    "                              padding =\"SAME\",training_able=self.training_able[1],init_filter = reader.get_tensor(\"filter_2\"),\n",
    "                             init_beta = reader.get_tensor(\"beta2\"),init_gama = reader.get_tensor(\"gama2\"))\n",
    "        print(\"conv_2 shape is:\",conv_2.shape) # (None,208,208,64)\n",
    "        conv_3 = self.my_conv(name=\"filter_3\", shape=[1, 1, 64, 32], input_data=conv_2, strides=[1, 1, 1, 1],\n",
    "                              padding=\"VALID\",training_able=self.training_able[2],init_filter = reader.get_tensor(\"filter_3\"),\n",
    "                             init_beta = reader.get_tensor(\"beta3\"),init_gama = reader.get_tensor(\"gama3\"))\n",
    "        print(\"conv_3 shape is:\", conv_3.shape) # (None,208,208,32)\n",
    "        conv_4 = self.my_conv(name=\"filter_4\", shape=[3, 3, 32, 64], input_data=conv_3, strides=[1, 1, 1, 1],\n",
    "                              padding=\"SAME\",training_able=self.training_able[3],init_filter = reader.get_tensor(\"filter_4\"),\n",
    "                             init_beta = reader.get_tensor(\"beta4\"),init_gama = reader.get_tensor(\"gama4\"))\n",
    "        print(\"conv_4 shape is:\", conv_4.shape) # (None,208,208,64)\n",
    "        resi_1 = conv_2 + conv_4 # 残差项\n",
    "        conv_5 = self.my_conv(name=\"filter_5\", shape=[3, 3, 64, 128], input_data=resi_1, strides=[1, 2, 2, 1],\n",
    "                              padding=\"SAME\",training_able=self.training_able[4],init_filter = reader.get_tensor(\"filter_5\"),\n",
    "                             init_beta = reader.get_tensor(\"beta5\"),init_gama = reader.get_tensor(\"gama5\"))\n",
    "        print(\"conv_5 shape is:\", conv_5.shape) # (None,104,104,128)\n",
    "        conv_6 = self.my_conv(name = \"filter_6\", shape =[1,1,128,64], input_data=conv_5, strides=[1,1,1,1],\n",
    "                              padding=\"SAME\",training_able=self.training_able[5],init_filter = reader.get_tensor(\"filter_6\"),\n",
    "                             init_beta = reader.get_tensor(\"beta6\"),init_gama = reader.get_tensor(\"gama6\"))\n",
    "        print(\"conv_6 shape is:\",conv_6.shape) # (None,104,104,64)\n",
    "        conv_7 = self.my_conv(name=\"filter_7\", shape=[3, 3, 64, 128], input_data=conv_6, strides=[1, 1, 1, 1],\n",
    "                              padding=\"SAME\",training_able=self.training_able[6],init_filter = reader.get_tensor(\"filter_7\"),\n",
    "                             init_beta = reader.get_tensor(\"beta7\"),init_gama = reader.get_tensor(\"gama7\")) # (None,64,64,128)\n",
    "        print(\"conv_7 shape is:\", conv_7.shape) # (None,104,104,128)\n",
    "        resi_2 = conv_7 +conv_5\n",
    "\n",
    "        conv_8 =self.my_conv(name =\"filter_8\",shape=[3,3,128,64],input_data=resi_2,strides=[1,2,2,1],\n",
    "                             padding=\"SAME\",training_able=self.training_able[7],init_filter = reader.get_tensor(\"filter_8\"),\n",
    "                            init_beta = reader.get_tensor(\"beta8\"),init_gama = reader.get_tensor(\"gama8\"))\n",
    "        print(\"conv_8 shape is:\",conv_8.shape) # (None,52,52,64)\n",
    "        conv_9 = self.my_conv(name=\"filter_9\", shape=[3, 3, 64, 64], input_data=conv_8, strides=[1, 2, 2, 1],\n",
    "                              padding=\"SAME\",training_able=self.training_able[8],init_filter = reader.get_tensor(\"filter_9\"),\n",
    "                             init_beta = reader.get_tensor(\"beta9\"),init_gama = reader.get_tensor(\"gama9\"))\n",
    "        print(\"conv_9 shape is:\", conv_9.shape)  # (None,26,26,64)\n",
    "        \"\"\"\n",
    "        conv_10 = self.my_conv(name=\"filter_10\", shape=[3, 3, 16, 14], input_data=resi_2, strides=[1, 2, 2, 1],\n",
    "                              padding=\"SAME\")\n",
    "        print(\"conv_10 shape is:\", conv_10.shape)  # (None,8,8,14)\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        return conv_9 ,conv_8\n",
    "\n",
    "    def yolo_head_for_retrain(self,extract_result,skip_connection,reader):\n",
    "        conv_10 = self.my_conv_no_bn(name=\"filter_10\", shape=None, input_data=extract_result, strides=[1, 1, 1, 1],\n",
    "                              padding=\"SAME\",init_filter = reader.get_tensor(\"filter_10\"),training_able = self.training_able[9])    # (None,26,26,64)\n",
    "        print(\"conv_10 shape is:\", conv_10.shape)\n",
    "        conv_11 = self.my_conv_no_bn(name=\"filter_11\",shape=None,input_data=conv_10,strides=[1,1,1,1],\n",
    "                               padding=\"SAME\",init_filter = reader.get_tensor(\"filter_11\"),training_able = self.training_able[10])   # (None,26,26,64)\n",
    "        print(\"conv_11 shape is:\", conv_11.shape)\n",
    "        #conv_skip_con = self.my_conv(name = \"skipfilter_01\",shape = [1,1,64,64],input_data = skip_connection,strides=[1,1,1,1],\n",
    "                                     #padding=\"SAME\")\n",
    "        #to_depth_skip_con = tf.space_to_depth(conv_skip_con,2) # (None,26,26,256)\n",
    "        #reorg = tf.concat(axis = -1,values = [conv_11,to_depth_skip_con])    # (None,26,26,320)\n",
    "        #print(\"reorg.shape is:\",reorg.shape)\n",
    "        conv_12 = self.my_conv_no_bn(name=\"filter_12\",shape=None,input_data=conv_11,strides=[1,1,1,1],\n",
    "                               padding=\"SAME\",init_filter = reader.get_tensor(\"filter_12\"),training_able = self.training_able[11])  # (None,26,26,128)\n",
    "        print(\"conv_12 shape is:\", conv_12.shape)\n",
    "        conv_13 = self.my_conv_no_bn(name=\"filter_13\", shape=None, input_data=conv_12, strides=[1, 2, 2, 1],\n",
    "                               padding=\"SAME\",init_filter = reader.get_tensor(\"filter_13\"),training_able = self.training_able[12])  # (None,13,13,128)\n",
    "        print(\"conv_13 shape is:\", conv_13.shape)\n",
    "        conv_skip_con = self.my_conv_no_bn(name = \"skipfilter_01\",shape = None,input_data = skip_connection,strides=[1,1,1,1],\n",
    "                                     padding=\"SAME\",init_filter = reader.get_tensor(\"skipfilter_01\"),training_able = self.training_able[13])\n",
    "        to_depth_skip_con = tf.space_to_depth(conv_skip_con,4) # (None,13,13,1024)\n",
    "        reorg = tf.concat(axis = -1,values = [conv_13,to_depth_skip_con])    # (None,13,13,1024+128)\n",
    "        print(\"reorg.shape is:\",reorg.shape)\n",
    "        #conv_14 = self.my_conv(name = \"filter_14\",shape = [1,1,64,64],input_data = skip_connection,strides = [1,1,1,1],padding = \"SAME\")\n",
    "        \n",
    "        filter_15 = tf.get_variable(name=\"filter_15\",initializer=reader.get_tensor(\"filter_15\"),\n",
    "                                   trainable = self.training_able[14])\n",
    "        self.save_list.append(filter_15)\n",
    "        conv_15 = tf.nn.conv2d(reorg, filter_15, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        # (None,13,13,?)\n",
    "        # (None,13,13,?)\n",
    "        print(\"conv_15 shape is:\", conv_15.shape)\n",
    "        return conv_15\n",
    "    def prediction(self, reader, sample,num_classes, anchor_box_tensor, score_threshold =0.5,iou_threshold=0.6):\n",
    "        sample_placeholder = tf.placeholder(name=\"sample\", shape=[ 1,self.img_height, self.img_width, 3], dtype=tf.float32)\n",
    "        conv_9,conv_8 = self.backbone_for_pred(reader,sample_placeholder)\n",
    "        \n",
    "        conv_14 = self.yolo_head_for_pred(conv_9,conv_8,reader)\n",
    "        \n",
    "        print(\"conv_14.shape\",conv_14.shape)\n",
    "        box_xy, box_wh, box_confidence, class_probability = self.detection(conv_14, num_calsses=num_classes,\n",
    "                                                                           anchor_box_tensor=anchor_box_tensor)\n",
    "        print(\"box_xy.shape is :\",box_xy.shape)\n",
    "        pred_loc = tf.concat([box_xy[...,1:2] - box_wh[...,1:2]*0.5,\n",
    "                              box_xy[..., 0:1] - box_wh[..., 0:1] * 0.5,\n",
    "                              box_xy[...,1:2] + box_wh[...,1:2]*0.5,\n",
    "                              box_xy[..., 0:1] + box_wh[..., 0:1] * 0.5,\n",
    "                               ], axis=-1)                                   # 传进non_max_suppression时，location是先行，后列，故先y,后x\n",
    "                                                                             # shape (batch, grid, grid,num_anchor_per_cell,4)\n",
    "        print(pred_loc.shape)\n",
    "        pred_loc = tf.minimum(tf.maximum(pred_loc,0),1)  # 让其落在0,1之间\n",
    "        temp = pred_loc\n",
    "        # 将坐标转换到像素级别\n",
    "        height_width_height_width = tf.cast(tf.expand_dims(tf.stack([self.img_height, self.img_width]*2,axis = 0),axis=0),tf.float32)\n",
    "        print(\"height_width_height_width shape is:\",height_width_height_width.shape)\n",
    "        print(\"pred_loc shape is:\",pred_loc.shape)\n",
    "        pred_loc = tf.reshape(pred_loc,[-1,4])*height_width_height_width    # shape (batch*grid*grid*num_anchor_per_cell,4)\n",
    "        #pred_loc = tf.cast(pred_loc,dtype = tf.int32)\n",
    "        pred_conf = tf.reshape(box_confidence, shape=[-1])                  # shape (batch*grid*grid*num_anchor_per_cell,)\n",
    "        pred_prob = tf.reshape(class_probability, [-1, num_classes])            # shape (batch*grid*grid*num_anchor_per_cell,classes)\n",
    "        box_scores = tf.expand_dims(pred_conf, axis=1) * pred_prob          # confidence * probability_class  shape(batch*grid*grid*num_anchor_per_cell,classes)\n",
    "        box_label = tf.argmax(box_scores, axis=-1)                          # shape (batch*grid*grid*num_anchor_per_cell,)\n",
    "        box_scores_max = tf.reduce_max(box_scores, axis=-1)                 # shape (batch*grid*grid*num_anchor_per_cell,)\n",
    "        pred_mask = box_scores_max > score_threshold                        # shape (batch*grid*grid*num_anchor_per_cell,)\n",
    "        boxes = tf.boolean_mask(pred_loc, pred_mask)                        # shape (unknown,4)， 因为pred_mask中mask对应的pred_loc的数据会被抛弃\n",
    "        scores = tf.boolean_mask(box_scores_max, pred_mask)                 # shape  (unknown,)\n",
    "        pred_classes = tf.boolean_mask(box_label, pred_mask)                # shape  (unknown,)\n",
    "        idx_nms = tf.image.non_max_suppression(boxes, scores,\n",
    "                                               max_output_size=8,\n",
    "                                               iou_threshold=iou_threshold)\n",
    "        boxes = tf.gather(boxes, idx_nms)\n",
    "        scores = tf.gather(scores, idx_nms)\n",
    "        classes = tf.gather(pred_classes, idx_nms)\n",
    "        print(\"boxes.shape is:\",boxes.shape)\n",
    "        print(\"scores shape is:\",scores.shape)\n",
    "        print(\"classes shape is :\",classes.shape)\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            fin_boxes , fin_scores, fin_classes ,temp_value = sess.run([boxes,scores,classes,temp],feed_dict={sample_placeholder:sample/255})\n",
    "        return fin_boxes, fin_scores, fin_classes ,temp_value\n",
    "    def draw_rectangle(self,boxes, scores, classes,img,class_dict):\n",
    "        \"\"\"\n",
    "\n",
    "        :param boxes:  (?, 4) 均为正整数\n",
    "        :param scores: (?,)\n",
    "        :param classes:  (?,) 为目标类的序号\n",
    "        :param img:      (416,416,3) 正整数\n",
    "        :param class_dict:  以 目标类序号为键，目标类名为值的字典\n",
    "        :return:          nothing\n",
    "        \"\"\"\n",
    "        \n",
    "        color_list = [(0,0,255),(0,255,0),(255,0,0)]\n",
    "        boxes = boxes.astype(np.int32)\n",
    "        \n",
    "        for i in range(boxes.shape[0]):\n",
    "            now_color = color_list[classes[i]]\n",
    "            cv2.rectangle(img,(boxes[i,1],boxes[i,0]),(boxes[i,3],boxes[i,2]),now_color)\n",
    "            #draw_obj.text([boxes[i,1]+2,boxes[i,0]+2,],class_dict[classes[i]]+str(scores[i]), now_color)\n",
    "        \n",
    "        return img\n",
    "    def backbone_for_pred(self,reader,sample_placeholder):\n",
    "        conv_1 = self.my_conv(name = \"filter_1\",shape = [3,3,3,32],input_data = sample_placeholder, strides = [1,1,1,1], \n",
    "                              padding =\"SAME\",training_able=False ,init_filter = reader.get_tensor(\"filter_1\"),\n",
    "                             init_beta = reader.get_tensor(\"beta1\"),init_gama = reader.get_tensor(\"gama1\"))\n",
    "        print(\"conv_1 shpae is :\",conv_1.shape) # (None,416,416,32)\n",
    "        conv_2 = self.my_conv(name = \"filter_2\",shape = [3,3,32,64],input_data = conv_1, strides = [1,2,2,1],\n",
    "                              padding =\"SAME\",training_able=False,init_filter = reader.get_tensor(\"filter_2\"),\n",
    "                             init_beta = reader.get_tensor(\"beta2\"),init_gama = reader.get_tensor(\"gama2\"))\n",
    "        print(\"conv_2 shape is:\",conv_2.shape) # (None,208,208,64)\n",
    "        conv_3 = self.my_conv(name=\"filter_3\", shape=[1, 1, 64, 32], input_data=conv_2, strides=[1, 1, 1, 1],\n",
    "                              padding=\"VALID\",training_able=False,init_filter = reader.get_tensor(\"filter_3\"),\n",
    "                             init_beta = reader.get_tensor(\"beta3\"),init_gama = reader.get_tensor(\"gama3\"))\n",
    "        print(\"conv_3 shape is:\", conv_3.shape) # (None,208,208,32)\n",
    "        conv_4 = self.my_conv(name=\"filter_4\", shape=[3, 3, 32, 64], input_data=conv_3, strides=[1, 1, 1, 1],\n",
    "                              padding=\"SAME\",training_able=False,init_filter = reader.get_tensor(\"filter_4\"),\n",
    "                             init_beta = reader.get_tensor(\"beta4\"),init_gama = reader.get_tensor(\"gama4\"))\n",
    "        print(\"conv_4 shape is:\", conv_4.shape) # (None,208,208,64)\n",
    "        resi_1 = conv_2 + conv_4 # 残差项\n",
    "        conv_5 = self.my_conv(name=\"filter_5\", shape=[3, 3, 64, 128], input_data=resi_1, strides=[1, 2, 2, 1],\n",
    "                              padding=\"SAME\",training_able=False,init_filter = reader.get_tensor(\"filter_5\"),\n",
    "                             init_beta = reader.get_tensor(\"beta5\"),init_gama = reader.get_tensor(\"gama5\"))\n",
    "        print(\"conv_5 shape is:\", conv_5.shape) # (None,104,104,128)\n",
    "        conv_6 = self.my_conv(name = \"filter_6\", shape =[1,1,128,64], input_data=conv_5, strides=[1,1,1,1],\n",
    "                              padding=\"SAME\",training_able=False,init_filter = reader.get_tensor(\"filter_6\"),\n",
    "                             init_beta = reader.get_tensor(\"beta6\"),init_gama = reader.get_tensor(\"gama6\"))\n",
    "        print(\"conv_6 shape is:\",conv_6.shape) # (None,104,104,64)\n",
    "        conv_7 = self.my_conv(name=\"filter_7\", shape=[3, 3, 64, 128], input_data=conv_6, strides=[1, 1, 1, 1],\n",
    "                              padding=\"SAME\",training_able=False,init_filter = reader.get_tensor(\"filter_7\"),\n",
    "                             init_beta = reader.get_tensor(\"beta7\"),init_gama = reader.get_tensor(\"gama7\")) # (None,64,64,128)\n",
    "        print(\"conv_7 shape is:\", conv_7.shape) # (None,104,104,128)\n",
    "        resi_2 = conv_7 +conv_5\n",
    "\n",
    "        conv_8 =self.my_conv(name =\"filter_8\",shape=[3,3,128,64],input_data=resi_2,strides=[1,2,2,1],\n",
    "                             padding=\"SAME\",training_able=False,init_filter = reader.get_tensor(\"filter_8\"),\n",
    "                            init_beta = reader.get_tensor(\"beta8\"),init_gama = reader.get_tensor(\"gama8\"))\n",
    "        print(\"conv_8 shape is:\",conv_8.shape) # (None,52,52,64)\n",
    "        conv_9 = self.my_conv(name=\"filter_9\", shape=[3, 3, 64, 64], input_data=conv_8, strides=[1, 2, 2, 1],\n",
    "                              padding=\"SAME\",training_able=False,init_filter = reader.get_tensor(\"filter_9\"),\n",
    "                             init_beta = reader.get_tensor(\"beta9\"),init_gama = reader.get_tensor(\"gama9\"))\n",
    "        print(\"conv_9 shape is:\", conv_9.shape)  # (None,26,26,64)\n",
    "        \"\"\"\n",
    "        conv_10 = self.my_conv(name=\"filter_10\", shape=[3, 3, 16, 14], input_data=resi_2, strides=[1, 2, 2, 1],\n",
    "                              padding=\"SAME\")\n",
    "        print(\"conv_10 shape is:\", conv_10.shape)  # (None,8,8,14)\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        return conv_9 ,conv_8\n",
    "\n",
    "    def yolo_head_for_pred(self, extract_result,skip_connection, reader):\n",
    "        conv_10 = self.my_conv_no_bn(name=\"filter_10\", shape=None, input_data=extract_result, strides=[1, 1, 1, 1],\n",
    "                              padding=\"SAME\",init_filter = reader.get_tensor(\"filter_10\"),training_able = False)    # (None,26,26,64)\n",
    "        print(\"conv_10 shape is:\", conv_10.shape)\n",
    "        conv_11 = self.my_conv_no_bn(name=\"filter_11\",shape=None,input_data=conv_10,strides=[1,1,1,1],\n",
    "                               padding=\"SAME\",init_filter = reader.get_tensor(\"filter_11\"),training_able = False)   # (None,26,26,64)\n",
    "        print(\"conv_11 shape is:\", conv_11.shape)\n",
    "        #conv_skip_con = self.my_conv(name = \"skipfilter_01\",shape = [1,1,64,64],input_data = skip_connection,strides=[1,1,1,1],\n",
    "                                     #padding=\"SAME\")\n",
    "        #to_depth_skip_con = tf.space_to_depth(conv_skip_con,2) # (None,26,26,256)\n",
    "        #reorg = tf.concat(axis = -1,values = [conv_11,to_depth_skip_con])    # (None,26,26,320)\n",
    "        #print(\"reorg.shape is:\",reorg.shape)\n",
    "        conv_12 = self.my_conv_no_bn(name=\"filter_12\",shape=None,input_data=conv_11,strides=[1,1,1,1],\n",
    "                               padding=\"SAME\",init_filter = reader.get_tensor(\"filter_12\"),training_able = False)  # (None,26,26,128)\n",
    "        print(\"conv_12 shape is:\", conv_12.shape)\n",
    "        conv_13 = self.my_conv_no_bn(name=\"filter_13\", shape=None, input_data=conv_12, strides=[1, 2, 2, 1],\n",
    "                               padding=\"SAME\",init_filter = reader.get_tensor(\"filter_13\"),training_able = False)  # (None,13,13,128)\n",
    "        print(\"conv_13 shape is:\", conv_13.shape)\n",
    "        conv_skip_con = self.my_conv_no_bn(name = \"skipfilter_01\",shape = None,input_data = skip_connection,strides=[1,1,1,1],\n",
    "                                     padding=\"SAME\",init_filter = reader.get_tensor(\"skipfilter_01\"),training_able = False)\n",
    "        to_depth_skip_con = tf.space_to_depth(conv_skip_con,4) # (None,13,13,1024)\n",
    "        reorg = tf.concat(axis = -1,values = [conv_13,to_depth_skip_con])    # (None,13,13,1024+128)\n",
    "        print(\"reorg.shape is:\",reorg.shape)\n",
    "        #conv_14 = self.my_conv(name = \"filter_14\",shape = [1,1,64,64],input_data = skip_connection,strides = [1,1,1,1],padding = \"SAME\")\n",
    "        \n",
    "        filter_15 = tf.get_variable(name=\"filter_15\",initializer=reader.get_tensor(\"filter_15\"),trainable = False)\n",
    "        self.save_list.append(filter_15)\n",
    "        conv_15 = tf.nn.conv2d(reorg, filter_15, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "      \n",
    "        # (None,13,13,?)\n",
    "        print(\"conv_15 shape is:\", conv_15.shape)\n",
    "        return conv_15\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anchor_box = np.array([[55.13103448, 29.12413793],\n",
    " [24.93540052, 30.69250646],\n",
    " [21.9784264 , 20.24365482],\n",
    " [15.75308642, 47.22222222],\n",
    " [30.5497076 , 52.80116959],\n",
    " [49.05109489, 13.37956204],\n",
    " [34.78171091, 23.25073746],\n",
    " [16.73464912, 15.02631579]],dtype=np.float32)/(np.array([416.0,416.0],dtype = np.float32).reshape(1,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images, anchor_labels, true_box_labels ,prior_boxes= readh5(r\"D:\\YOLOv3_2\\train_data2.h5\",amount = (0,550))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(550, 416, 416, 3)\n",
      "(550, 13, 13, 8, 8)\n",
      "(550, 10, 5)\n",
      "(550, 13, 13, 8, 4)\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)\n",
    "print(anchor_labels.shape)\n",
    "print(true_box_labels.shape)\n",
    "print(prior_boxes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_1 shpae is : (?, 416, 416, 32)\n",
      "conv_2 shape is: (?, 208, 208, 64)\n",
      "conv_3 shape is: (?, 208, 208, 32)\n",
      "conv_4 shape is: (?, 208, 208, 64)\n",
      "conv_5 shape is: (?, 104, 104, 128)\n",
      "conv_6 shape is: (?, 104, 104, 64)\n",
      "conv_7 shape is: (?, 104, 104, 128)\n",
      "conv_8 shape is: (?, 52, 52, 64)\n",
      "conv_9 shape is: (?, 26, 26, 64)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-d2f5c322b5f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mYolo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_true_boxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrue_box_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_anchor_per_box\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manchor_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimg_width\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimg_height\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtraining_able\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_able\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mone\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manchor_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrue_box_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprior_boxes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m401\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0manchor_box\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0manchor_box\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-c5a791661742>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, training_data, training_label, training_treu_boxes, training_prior_boxes, classes, learning_rate, minibatch_size, num_epochs, anchor_box, reader)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[0manchor_box_tensor\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manchor_box\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[0mconv_9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mconv_8\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_img\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_able\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[0mconv_15\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myolo_head\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv_9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mconv_8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m         \u001b[0mbox_xy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox_wh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox_confidence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_probability\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv_15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_calsses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0manchor_box_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manchor_box_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[0miou_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdetection_mask\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocess_true_box\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrue_boxes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbox_wh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbox_xy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_label\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mthreshold_iou\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-c5a791661742>\u001b[0m in \u001b[0;36myolo_head\u001b[1;34m(self, extract_result, skip_connection, classes)\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0myolo_head\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mextract_result\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mskip_connection\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m         conv_10 = self.my_conv_no_bn(name=\"filter_10\", shape=[1, 1, 64, 64], input_data=extract_result, strides=[1, 1, 1, 1],\n\u001b[1;32m--> 192\u001b[1;33m                               padding=\"SAME\")    # (None,26,26,64)\n\u001b[0m\u001b[0;32m    193\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"conv_10 shape is:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv_10\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         conv_11 = self.my_conv_no_bn(name=\"filter_11\",shape=[3,3,64,64],input_data=conv_10,strides=[1,1,1,1],\n",
      "\u001b[1;32m<ipython-input-7-c5a791661742>\u001b[0m in \u001b[0;36mmy_conv_no_bn\u001b[1;34m(self, name, shape, input_data, strides, padding, training_able, init_filter, skip_item)\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minit_filter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             filter_ = tf.get_variable(name=name, shape=shape, dtype=tf.float32,\n\u001b[1;32m---> 59\u001b[1;33m                                       \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxavier_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m                                       trainable=training_able)\n\u001b[0;32m     61\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\site-packages\\tensorflow\\python\\util\\lazy_loader.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\site-packages\\tensorflow\\python\\util\\lazy_loader.py\u001b[0m in \u001b[0;36m_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     40\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;31m# Import the target module and insert it into the parent's namespace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_module_globals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_local_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\importlib\\__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\site-packages\\tensorflow\\contrib\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcopy_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcrf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcudnn_rnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\site-packages\\tensorflow\\contrib\\cudnn_rnn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m# pylint: disable=unused-import,wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn_rnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;31m# pylint: enable=unused-import,wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\site-packages\\tensorflow\\contrib\\cudnn_rnn\\python\\layers\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# pylint: disable=unused-import,wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn_rnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn_rnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;31m# pylint: enable=unused-import,wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\site-packages\\tensorflow\\contrib\\cudnn_rnn\\python\\layers\\cudnn_rnn.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn_rnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcudnn_rnn_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\site-packages\\tensorflow\\contrib\\cudnn_rnn\\python\\ops\\cudnn_rnn_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msplit_dependency\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlstm_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\site-packages\\tensorflow\\contrib\\rnn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm_ops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn_cell\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_best_effort_input_batch_size\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mbest_effort_input_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\site-packages\\tensorflow\\contrib\\rnn\\python\\ops\\rnn_cell.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompiler\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjit\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcore_rnn_cell\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\site-packages\\tensorflow\\contrib\\layers\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;31m# pylint: disable=unused-import,wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m \u001b[1;31m# pylint: enable=unused-import,wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrev_block_lib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummaries\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_column\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbucketization_op\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse_feature_cross_op\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\target_column.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mloss_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetric_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\site-packages\\tensorflow\\contrib\\losses\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetric_learning\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\metric_learning\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetric_learning\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetric_loss_ops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_util\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mremove_undocumented\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\metric_learning\\metric_loss_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m   \u001b[1;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m   \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m   \u001b[0mHAS_SKLEARN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\site-packages\\sklearn\\metrics\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mranking\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mauc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mranking\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maverage_precision_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mranking\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcoverage_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\site-packages\\sklearn\\metrics\\ranking.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrankdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0massert_all_finite\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\site-packages\\scipy\\stats\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdivision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmorestats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\site-packages\\scipy\\stats\\stats.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mspecial\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmstats_basic\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_distn_infrastructure\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_lazywhere\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\site-packages\\scipy\\stats\\distributions.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdivision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m from ._distn_infrastructure import (entropy, rv_discrete, rv_continuous,\n\u001b[0m\u001b[0;32m     11\u001b[0m                                     rv_frozen)\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# for root finding for discrete distribution ppf, and max likelihood estimation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moptimize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# for functions of continuous distributions (e.g. moments, entropy, cdf)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\site-packages\\scipy\\optimize\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdivision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 270\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    271\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_minimize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_root\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    training_able = [False]*6+[True]*3\n",
    "    reader = pywrap_tensorflow.NewCheckpointReader(r'D:\\YOLOv3\\classification save4\\model.ckpt-120')\n",
    "    tf.reset_default_graph()\n",
    "    one = Yolo(num_true_boxes = true_box_labels.shape[1],num_anchor_per_box = anchor_labels.shape[3],img_width=train_images.shape[2],img_height=train_images.shape[1],training_able=training_able)\n",
    "    one.train(train_images, anchor_labels,true_box_labels, prior_boxes,classes = 3, learning_rate=0.001, minibatch_size=7, num_epochs=401,anchor_box=anchor_box,reader=reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_1 shpae is : (?, 416, 416, 32)\n",
      "conv_2 shape is: (?, 208, 208, 64)\n",
      "conv_3 shape is: (?, 208, 208, 32)\n",
      "conv_4 shape is: (?, 208, 208, 64)\n",
      "conv_5 shape is: (?, 104, 104, 128)\n",
      "conv_6 shape is: (?, 104, 104, 64)\n",
      "conv_7 shape is: (?, 104, 104, 128)\n",
      "conv_8 shape is: (?, 52, 52, 64)\n",
      "conv_9 shape is: (?, 26, 26, 64)\n",
      "conv_10 shape is: (?, 26, 26, 64)\n",
      "conv_11 shape is: (?, 26, 26, 64)\n",
      "conv_12 shape is: (?, 26, 26, 128)\n",
      "conv_13 shape is: (?, 13, 13, 128)\n",
      "reorg.shape is: (?, 13, 13, 1152)\n",
      "conv_15 shape is: (?, 13, 13, 64)\n",
      "(?, 2)\n",
      "box_confidence shape is (?, 13, 13, 8, 1)\n",
      "detection_mask is: (?, 13, 13, 8, 1)\n",
      "10.424698035304363\n",
      "7.428769316691428\n",
      "7.3274839874834585\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-91ba206f7207>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mYolo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_true_boxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrue_box_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_anchor_per_box\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manchor_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimg_width\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimg_height\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtraining_able\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_able\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mone\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manchor_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrue_box_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprior_boxes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m91\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0manchor_box\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0manchor_box\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-4f4af2218e93>\u001b[0m in \u001b[0;36mretrain\u001b[1;34m(self, training_data, training_anchor_label, training_treu_boxes, training_prior_boxes, classes, learning_rate, minibatch_size, num_epochs, anchor_box, reader, lambd)\u001b[0m\n\u001b[0;32m    383\u001b[0m                     _, minibatch_cost = sess.run([optimizer, total_cost],\n\u001b[0;32m    384\u001b[0m                                                  feed_dict={input_img: batch[0] / 255, input_label: batch[1],\n\u001b[1;32m--> 385\u001b[1;33m                                                             true_boxes: batch[2],prior_boxes:batch[3]})\n\u001b[0m\u001b[0;32m    386\u001b[0m                     \u001b[0mepoch_cost\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mminibatch_cost\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m                 \u001b[0mepoch_cost\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mnum_minibatches\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\for_anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    training_able = [False]*0+[True]*15\n",
    "    reader = pywrap_tensorflow.NewCheckpointReader(r'D:\\YOLOv3_2\\test_samples_copy\\model.ckpt-20')\n",
    "    tf.reset_default_graph()\n",
    "    one = Yolo(num_true_boxes = true_box_labels.shape[1],num_anchor_per_box = anchor_labels.shape[3],img_width=train_images.shape[2],img_height=train_images.shape[1],training_able=training_able)\n",
    "    one.retrain(train_images, anchor_labels,true_box_labels, prior_boxes,classes = 3, learning_rate=0.001, minibatch_size=7, num_epochs=91,anchor_box=anchor_box,reader=reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reader = pywrap_tensorflow.NewCheckpointReader(r'D:\\YOLOv3_2\\test_samples_copy2\\model.ckpt-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "dict_class ={}\n",
    "dict_class[0]=\"ball\"\n",
    "dict_class[1]=\"chess\"\n",
    "dict_class[2]=\"pen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_1 shpae is : (1, 416, 416, 32)\n",
      "conv_2 shape is: (1, 208, 208, 64)\n",
      "conv_3 shape is: (1, 208, 208, 32)\n",
      "conv_4 shape is: (1, 208, 208, 64)\n",
      "conv_5 shape is: (1, 104, 104, 128)\n",
      "conv_6 shape is: (1, 104, 104, 64)\n",
      "conv_7 shape is: (1, 104, 104, 128)\n",
      "conv_8 shape is: (1, 52, 52, 64)\n",
      "conv_9 shape is: (1, 26, 26, 64)\n",
      "conv_10 shape is: (1, 26, 26, 64)\n",
      "conv_11 shape is: (1, 26, 26, 64)\n",
      "conv_12 shape is: (1, 26, 26, 128)\n",
      "conv_13 shape is: (1, 13, 13, 128)\n",
      "reorg.shape is: (1, 13, 13, 1152)\n",
      "conv_15 shape is: (1, 13, 13, 64)\n",
      "conv_14.shape (1, 13, 13, 64)\n",
      "(?, 2)\n",
      "box_confidence shape is (1, 13, 13, 8, 1)\n",
      "box_xy.shape is : (1, 13, 13, 8, 2)\n",
      "(1, 13, 13, 8, 4)\n",
      "height_width_height_width shape is: (1, 4)\n",
      "pred_loc shape is: (1, 13, 13, 8, 4)\n",
      "boxes.shape is: (?, 4)\n",
      "scores shape is: (?,)\n",
      "classes shape is : (?,)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sample = train_images[0].reshape((-1,)+train_images[0].shape)\n",
    "one = Yolo(num_true_boxes = true_box_labels.shape[1],num_anchor_per_box = anchor_labels.shape[3],img_width=train_images.shape[2],img_height=train_images.shape[1],training_able=None)\n",
    "fin_boxes, fin_scores, fin_classes,temp_value =one.prediction( reader, sample/255,num_classes=3, anchor_box_tensor=anchor_box, score_threshold =0.4,iou_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, 2, 0], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[163.3814 , 131.77171, 181.71129, 156.449  ],\n",
       "       [195.47214, 192.08481, 213.32152, 218.65622],\n",
       "       [ 28.21427, 101.97961, 105.02873, 114.80089],\n",
       "       [ 98.12455, 203.35565, 137.8103 , 214.23494],\n",
       "       [159.00377, 243.3138 , 173.31683, 265.02304]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.       , 0.9999243, 0.999912 , 0.9998685, 0.999859 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 13, 13, 8, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()\n",
    "                         \n",
    "img = one.draw_rectangle(fin_boxes, fin_scores, fin_classes,np.squeeze(sample,axis=0),dict_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(416, 416, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "cv2.imshow(\"es\",img)\n",
    "cv2.waitKey(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_rectangle(boxes, scores, classes,img,class_dict):\n",
    "        \"\"\"\n",
    "\n",
    "        :param boxes:  (?, 4) 均为正整数\n",
    "        :param scores: (?,)\n",
    "        :param classes:  (?,) 为目标类的序号\n",
    "        :param img:      (256,256,3) 正整数\n",
    "        :param class_dict:  以 目标类序号为键，目标类名为值的字典\n",
    "        :return:          nothing\n",
    "        \"\"\"\n",
    "        draw_obj = ImageDraw.Draw(img)\n",
    "        color_list = [\"red\",\"green\",\"blue\"]\n",
    "        boxes = boxes.astype(np.int32)\n",
    "        for i in range(1):\n",
    "            now_color = color_list[classes[i]]\n",
    "            draw_obj.rectangle([boxes[i,1],boxes[i,0],boxes[i,3],boxes[i,2]],outline = now_color)\n",
    "            #draw_obj.text([boxes[i,1]+2,boxes[i,0]+2,],class_dict[classes[i]]+str(scores[i]), now_color)\n",
    "        del draw_obj\n",
    "        return np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = np.array([[ 20 , 60      , 40 , 80 ],\n",
    "       [ 100, 120 ,  130,  150]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = draw_rectangle(temp, fin_scores, fin_classes, Image.fromarray(np.squeeze(sample,axis=0)),class_dict= dict_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 8, 8, 8, 8, 9, 9, 9, 9],\n",
       "       dtype=int64),\n",
       " array([5, 5, 5, 5, 3, 3, 3, 3, 6, 6, 6, 6, 5, 5, 5, 5, 2, 2, 2, 2],\n",
       "       dtype=int64),\n",
       " array([2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 1, 1, 1, 1, 3, 3, 3, 3],\n",
       "       dtype=int64),\n",
       " array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3],\n",
       "       dtype=int64))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nonzero(prior_boxes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
